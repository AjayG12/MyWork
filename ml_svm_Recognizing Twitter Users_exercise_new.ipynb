{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os\n",
    "#\n",
    "## os.chdir( write data directory path)\n",
    "\n",
    "# turn of data table rendering\n",
    "#sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "Support Vector Machines, or SVMs, are supervised learning models used for classification and regression analysis.  Given a data set with features and classes, an SVM finds a line with the largest distance between each of two features, called an optimal hyperplane with a maximal margin. It's like a highway through the data points separating the two features as much as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel functions\n",
    "The example here uses a linear kernel function. Applied to the features it makes the data highly separable by increasing the dimensionality of the data. The scikit-learn SVM implementation had three [built-in kernel functions](http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html) 'linear', 'poly' and 'rbf'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "In this example we use an SVM with a linear kernel to predict who wrote an update on Twitter. The dataset contains recent tweets of two data analytics giants; Gregory Piatetsky ([@kdnuggets](https://twitter.com/kdnuggets)) and Big Data Science ([@analyticbridge](https://twitter.com/analyticbridge)). The screen_name acts as the label while each word in the tweet is a feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 200 tweets from @kdnuggets and @analyticbridge (n=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show arbitrary datapoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      screen_name                                              tweet\n",
       "0  analyticbridge  Six keywords characterizing milestones in the ...\n",
       "1       kdnuggets  Get practical training at PASS Business Analyt...\n",
       "2  analyticbridge  The application of Propensity Score Matching h...\n",
       "3       kdnuggets  Old data analysts don't die - they just get br...\n",
       "4       kdnuggets  How Many Quants are Changing Jobs? http://t.co..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the data\n",
    "def shuffle(data, n):\n",
    "    ind = data.index\n",
    "    for i in range(n):\n",
    "       sampler = np.random.permutation(data.shape[0])\n",
    "       new_vals = data.take(sampler).values\n",
    "       data = pd.DataFrame(new_vals, index=ind)\n",
    "    return data\n",
    "\n",
    "df = shuffle(df, 1)\n",
    "df.columns = ['screen_name', 'tweet']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## easy way to shuffle ?? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test sets\n",
    "To train our SVM model, we need a large enough training set and a test set to test for accuracy. We split the data 75% for training and 25% for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training set (3/4 of the data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test set (1/4 of the data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectors  and TF-IDF Feature Vectors\n",
    "To extract useful features from the tweets we use the TF-IDF algorithm to qualify the importance of words. For more on TF-IDF, see [Using TF-IDF for Text Mining in Python](http://unsupervised-learning.com/tf-idf-text-mining-python). To do this, we use the [count_vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [tfidf_transformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) from the scikit-learn machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trsansform data using count vectorizer and tfidf and create 2 different train-test data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the SVM Classifier\n",
    "We train the Support Vector Machine with a linear kernel on the TF-IDF scores and both Twitter handles as the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVC with linear kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "To test our fitted model, we use the test set to predict tweets it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy\n",
    "We determine the current model accuracy by comparing the actual test labels with the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "#accuracy = accuracy_score(pred, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(classification_report(pred, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### try it using polynomial and rbf kerners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#svclassifier = SVC(kernel='poly', degree=8)\n",
    "#svclassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
